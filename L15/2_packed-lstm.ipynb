{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 453: Deep Learning (Spring 2021)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2021/  \n",
    "GitHub repository: https://github.com/rasbt/stat453-deep-learning-ss21\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY4SK0xKAJgm"
   },
   "source": [
    "# Same as 1_lstm.ipynb but with packed sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sc6xejhY-NzZ"
   },
   "source": [
    "Explanation of packing: https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moNmVfuvnImW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sebastian Raschka\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11\n",
      "IPython version      : 7.21.0\n",
      "\n",
      "torch    : 2.3.0\n",
      "torchtext: 0.18.0\n",
      "datasets : 3.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch,torchtext,datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSRL42Qgy8I8"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvW1RgfepCBq"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQMmKUEisW4W"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will download the IMDB movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/) for positive-negative sentiment classification in as CSV-formatted file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-12 22:05:05--  https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz [following]\n",
      "--2021-04-12 22:05:05--  https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch08/movie_data.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26521894 (25M) [application/octet-stream]\n",
      "Saving to: ‘movie_data.csv.gz’\n",
      "\n",
      "movie_data.csv.gz   100%[===================>]  25.29M  15.8MB/s    in 1.6s    \n",
      "\n",
      "2021-04-12 22:05:07 (15.8 MB/s) - ‘movie_data.csv.gz’ saved [26521894/26521894]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f movie_data.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dataset looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset with Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "!pip install -U spacy==3.7.3\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download English vocabulary via:\n",
    "    \n",
    "- `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"movie_data.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation, and test partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WZ_4jiHVnMxN",
    "outputId": "dfa51c04-4845-44c3-f50b-d36d41f132b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['review', 'sentiment'],\n",
       "     num_rows: 34000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['review', 'sentiment'],\n",
       "     num_rows: 6000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['review', 'sentiment'],\n",
       "     num_rows: 10000\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainvalid_test_dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "train_valid_dataset = trainvalid_test_dataset['train'].train_test_split(test_size=0.15)\n",
    "\n",
    "train_data, valid_data = train_valid_dataset['train'], train_valid_dataset['test']\n",
    "test_data = trainvalid_test_dataset['test']\n",
    "\n",
    "train_data, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 34000\n",
      "Num Valid: 6000\n",
      "Num Test: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Valid: {len(valid_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'I am a member of a canoeing club and I can tell you the truth that Deliverance is synonomous with the peacefulness and tranquility of the experience. As we put our boats into the water, banjoes echo in the back of the conscious mind. This movie is timeless because it waxes philosophical of human\\'s place in nature and technology\\'s effect upon man\\'s relationship with nature. We see it in the bow fishing. We see it in the home made tent. There is also city man\\'s disdain and feeling of superiority to the rural woodsman \"cracker\". The fact that the Banker from Atlanta (Ned Beatty) has \"bad teeth\" is meant to put him on the same level with the woodsmen who also have bad teeth. Ultimately, the struggle of life and death supersedes \"civilized man\\'s\" suppositives about \"The Law\". This canoe trip ends too soon for the viewer, but alas Not Soon Enough for the characters.',\n",
       " 'sentiment': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(row, nlp, lower=True):\n",
    "    tokens = [token.text for token in nlp.tokenizer(row['review'])]\n",
    "    if lower:\n",
    "        tokens = [e.lower() for e in tokens]\n",
    "    return {'tokens': tokens, 'TEXT_LENGTH': len(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34000/34000 [00:31<00:00, 1081.32 examples/s]\n",
      "Map: 100%|██████████| 6000/6000 [00:04<00:00, 1248.14 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:07<00:00, 1268.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fn_kwargs={\n",
    "    'nlp': nlp,\n",
    "    'lower': True\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_fn, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_fn, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_fn, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'I am a member of a canoeing club and I can tell you the truth that Deliverance is synonomous with the peacefulness and tranquility of the experience. As we put our boats into the water, banjoes echo in the back of the conscious mind. This movie is timeless because it waxes philosophical of human\\'s place in nature and technology\\'s effect upon man\\'s relationship with nature. We see it in the bow fishing. We see it in the home made tent. There is also city man\\'s disdain and feeling of superiority to the rural woodsman \"cracker\". The fact that the Banker from Atlanta (Ned Beatty) has \"bad teeth\" is meant to put him on the same level with the woodsmen who also have bad teeth. Ultimately, the struggle of life and death supersedes \"civilized man\\'s\" suppositives about \"The Law\". This canoe trip ends too soon for the viewer, but alas Not Soon Enough for the characters.',\n",
       " 'sentiment': 1,\n",
       " 'tokens': ['i',\n",
       "  'am',\n",
       "  'a',\n",
       "  'member',\n",
       "  'of',\n",
       "  'a',\n",
       "  'canoeing',\n",
       "  'club',\n",
       "  'and',\n",
       "  'i',\n",
       "  'can',\n",
       "  'tell',\n",
       "  'you',\n",
       "  'the',\n",
       "  'truth',\n",
       "  'that',\n",
       "  'deliverance',\n",
       "  'is',\n",
       "  'synonomous',\n",
       "  'with',\n",
       "  'the',\n",
       "  'peacefulness',\n",
       "  'and',\n",
       "  'tranquility',\n",
       "  'of',\n",
       "  'the',\n",
       "  'experience',\n",
       "  '.',\n",
       "  'as',\n",
       "  'we',\n",
       "  'put',\n",
       "  'our',\n",
       "  'boats',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  ',',\n",
       "  'banjoes',\n",
       "  'echo',\n",
       "  'in',\n",
       "  'the',\n",
       "  'back',\n",
       "  'of',\n",
       "  'the',\n",
       "  'conscious',\n",
       "  'mind',\n",
       "  '.',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'timeless',\n",
       "  'because',\n",
       "  'it',\n",
       "  'waxes',\n",
       "  'philosophical',\n",
       "  'of',\n",
       "  'human',\n",
       "  \"'s\",\n",
       "  'place',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'and',\n",
       "  'technology',\n",
       "  \"'s\",\n",
       "  'effect',\n",
       "  'upon',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'relationship',\n",
       "  'with',\n",
       "  'nature',\n",
       "  '.',\n",
       "  'we',\n",
       "  'see',\n",
       "  'it',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bow',\n",
       "  'fishing',\n",
       "  '.',\n",
       "  'we',\n",
       "  'see',\n",
       "  'it',\n",
       "  'in',\n",
       "  'the',\n",
       "  'home',\n",
       "  'made',\n",
       "  'tent',\n",
       "  '.',\n",
       "  'there',\n",
       "  'is',\n",
       "  'also',\n",
       "  'city',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'disdain',\n",
       "  'and',\n",
       "  'feeling',\n",
       "  'of',\n",
       "  'superiority',\n",
       "  'to',\n",
       "  'the',\n",
       "  'rural',\n",
       "  'woodsman',\n",
       "  '\"',\n",
       "  'cracker',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'the',\n",
       "  'banker',\n",
       "  'from',\n",
       "  'atlanta',\n",
       "  '(',\n",
       "  'ned',\n",
       "  'beatty',\n",
       "  ')',\n",
       "  'has',\n",
       "  '\"',\n",
       "  'bad',\n",
       "  'teeth',\n",
       "  '\"',\n",
       "  'is',\n",
       "  'meant',\n",
       "  'to',\n",
       "  'put',\n",
       "  'him',\n",
       "  'on',\n",
       "  'the',\n",
       "  'same',\n",
       "  'level',\n",
       "  'with',\n",
       "  'the',\n",
       "  'woodsmen',\n",
       "  'who',\n",
       "  'also',\n",
       "  'have',\n",
       "  'bad',\n",
       "  'teeth',\n",
       "  '.',\n",
       "  'ultimately',\n",
       "  ',',\n",
       "  'the',\n",
       "  'struggle',\n",
       "  'of',\n",
       "  'life',\n",
       "  'and',\n",
       "  'death',\n",
       "  'supersedes',\n",
       "  '\"',\n",
       "  'civilized',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  '\"',\n",
       "  'suppositives',\n",
       "  'about',\n",
       "  '\"',\n",
       "  'the',\n",
       "  'law',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'this',\n",
       "  'canoe',\n",
       "  'trip',\n",
       "  'ends',\n",
       "  'too',\n",
       "  'soon',\n",
       "  'for',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  ',',\n",
       "  'but',\n",
       "  'alas',\n",
       "  'not',\n",
       "  'soon',\n",
       "  'enough',\n",
       "  'for',\n",
       "  'the',\n",
       "  'characters',\n",
       "  '.'],\n",
       " 'TEXT_LENGTH': 182}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-TBwKWPslPa"
   },
   "source": [
    "Build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e8uNrjdtn4A8",
    "outputId": "6cf499d7-7722-4da0-8576-ee0f218cc6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36500\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "# Do not create an index for tokens which appear less than min_freq times in our training set.\n",
    "min_freq = 5\n",
    "\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token\n",
    "]\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "print(f'Vocabulary size: {len(en_vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The special tokens (`<unk>` and `<pad>`) are added into the vocabuary\n",
    "- PyTorch RNNs can deal with arbitrary lengths due to dynamic graphs, but padding is necessary for padding sequences to the same length in a given minibatch so we can store those in an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens corresponding to the first 10 indices (0, 1, ..., 9):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab.get_itos()[:10]) # itos = integer-to-string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting a string to an integer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab.get_stoi()['the']) # stoi = string-to-integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the `in` keyword to get a boolean indicating if a token is in the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'the' in en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The' in en_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `set_default_index` method we can set what value is returned when we try and get the index of a token outside of our vocabulary. In this case, the index of the unknown token, `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_index = en_vocab[unk_token]\n",
    "en_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can happily get indexes of out of vocabulary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[\"The\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert tokens into indices:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['they', 'are', 'very', 'unusual', 'in', 'a', 'period', 'drama', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 32, 63, 1756, 11, 6, 861, 466, 4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, we can use the `lookup_tokens` method to convert a list of indices back into tokens using the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'are', 'very', 'unusual', 'in', 'a', 'period', 'drama', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.lookup_tokens(en_vocab.lookup_indices(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_fn(row, vocab):\n",
    "    token_ids = vocab.lookup_indices(row[\"tokens\"])\n",
    "    return {\"token_ids\": token_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34000/34000 [00:14<00:00, 2304.65 examples/s]\n",
      "Map: 100%|██████████| 6000/6000 [00:02<00:00, 2412.62 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:04<00:00, 2351.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fn_kwargs = {\"vocab\": en_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_fn, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_fn, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_fn, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'I am a member of a canoeing club and I can tell you the truth that Deliverance is synonomous with the peacefulness and tranquility of the experience. As we put our boats into the water, banjoes echo in the back of the conscious mind. This movie is timeless because it waxes philosophical of human\\'s place in nature and technology\\'s effect upon man\\'s relationship with nature. We see it in the bow fishing. We see it in the home made tent. There is also city man\\'s disdain and feeling of superiority to the rural woodsman \"cracker\". The fact that the Banker from Atlanta (Ned Beatty) has \"bad teeth\" is meant to put him on the same level with the woodsmen who also have bad teeth. Ultimately, the struggle of life and death supersedes \"civilized man\\'s\" suppositives about \"The Law\". This canoe trip ends too soon for the viewer, but alas Not Soon Enough for the characters.',\n",
       " 'sentiment': 1,\n",
       " 'tokens': ['i',\n",
       "  'am',\n",
       "  'a',\n",
       "  'member',\n",
       "  'of',\n",
       "  'a',\n",
       "  'canoeing',\n",
       "  'club',\n",
       "  'and',\n",
       "  'i',\n",
       "  'can',\n",
       "  'tell',\n",
       "  'you',\n",
       "  'the',\n",
       "  'truth',\n",
       "  'that',\n",
       "  'deliverance',\n",
       "  'is',\n",
       "  'synonomous',\n",
       "  'with',\n",
       "  'the',\n",
       "  'peacefulness',\n",
       "  'and',\n",
       "  'tranquility',\n",
       "  'of',\n",
       "  'the',\n",
       "  'experience',\n",
       "  '.',\n",
       "  'as',\n",
       "  'we',\n",
       "  'put',\n",
       "  'our',\n",
       "  'boats',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  ',',\n",
       "  'banjoes',\n",
       "  'echo',\n",
       "  'in',\n",
       "  'the',\n",
       "  'back',\n",
       "  'of',\n",
       "  'the',\n",
       "  'conscious',\n",
       "  'mind',\n",
       "  '.',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'timeless',\n",
       "  'because',\n",
       "  'it',\n",
       "  'waxes',\n",
       "  'philosophical',\n",
       "  'of',\n",
       "  'human',\n",
       "  \"'s\",\n",
       "  'place',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'and',\n",
       "  'technology',\n",
       "  \"'s\",\n",
       "  'effect',\n",
       "  'upon',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'relationship',\n",
       "  'with',\n",
       "  'nature',\n",
       "  '.',\n",
       "  'we',\n",
       "  'see',\n",
       "  'it',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bow',\n",
       "  'fishing',\n",
       "  '.',\n",
       "  'we',\n",
       "  'see',\n",
       "  'it',\n",
       "  'in',\n",
       "  'the',\n",
       "  'home',\n",
       "  'made',\n",
       "  'tent',\n",
       "  '.',\n",
       "  'there',\n",
       "  'is',\n",
       "  'also',\n",
       "  'city',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'disdain',\n",
       "  'and',\n",
       "  'feeling',\n",
       "  'of',\n",
       "  'superiority',\n",
       "  'to',\n",
       "  'the',\n",
       "  'rural',\n",
       "  'woodsman',\n",
       "  '\"',\n",
       "  'cracker',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'the',\n",
       "  'banker',\n",
       "  'from',\n",
       "  'atlanta',\n",
       "  '(',\n",
       "  'ned',\n",
       "  'beatty',\n",
       "  ')',\n",
       "  'has',\n",
       "  '\"',\n",
       "  'bad',\n",
       "  'teeth',\n",
       "  '\"',\n",
       "  'is',\n",
       "  'meant',\n",
       "  'to',\n",
       "  'put',\n",
       "  'him',\n",
       "  'on',\n",
       "  'the',\n",
       "  'same',\n",
       "  'level',\n",
       "  'with',\n",
       "  'the',\n",
       "  'woodsmen',\n",
       "  'who',\n",
       "  'also',\n",
       "  'have',\n",
       "  'bad',\n",
       "  'teeth',\n",
       "  '.',\n",
       "  'ultimately',\n",
       "  ',',\n",
       "  'the',\n",
       "  'struggle',\n",
       "  'of',\n",
       "  'life',\n",
       "  'and',\n",
       "  'death',\n",
       "  'supersedes',\n",
       "  '\"',\n",
       "  'civilized',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  '\"',\n",
       "  'suppositives',\n",
       "  'about',\n",
       "  '\"',\n",
       "  'the',\n",
       "  'law',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'this',\n",
       "  'canoe',\n",
       "  'trip',\n",
       "  'ends',\n",
       "  'too',\n",
       "  'soon',\n",
       "  'for',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  ',',\n",
       "  'but',\n",
       "  'alas',\n",
       "  'not',\n",
       "  'soon',\n",
       "  'enough',\n",
       "  'for',\n",
       "  'the',\n",
       "  'characters',\n",
       "  '.'],\n",
       " 'token_ids': [12,\n",
       "  244,\n",
       "  6,\n",
       "  1727,\n",
       "  7,\n",
       "  6,\n",
       "  0,\n",
       "  1384,\n",
       "  5,\n",
       "  12,\n",
       "  72,\n",
       "  393,\n",
       "  26,\n",
       "  2,\n",
       "  884,\n",
       "  14,\n",
       "  7581,\n",
       "  9,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  26161,\n",
       "  7,\n",
       "  2,\n",
       "  599,\n",
       "  4,\n",
       "  20,\n",
       "  82,\n",
       "  286,\n",
       "  273,\n",
       "  9402,\n",
       "  97,\n",
       "  2,\n",
       "  1003,\n",
       "  3,\n",
       "  0,\n",
       "  9157,\n",
       "  11,\n",
       "  2,\n",
       "  160,\n",
       "  7,\n",
       "  2,\n",
       "  5528,\n",
       "  351,\n",
       "  4,\n",
       "  13,\n",
       "  23,\n",
       "  9,\n",
       "  3914,\n",
       "  98,\n",
       "  10,\n",
       "  28001,\n",
       "  4486,\n",
       "  7,\n",
       "  405,\n",
       "  16,\n",
       "  294,\n",
       "  11,\n",
       "  919,\n",
       "  5,\n",
       "  2192,\n",
       "  16,\n",
       "  942,\n",
       "  695,\n",
       "  141,\n",
       "  16,\n",
       "  659,\n",
       "  21,\n",
       "  919,\n",
       "  4,\n",
       "  82,\n",
       "  79,\n",
       "  10,\n",
       "  11,\n",
       "  2,\n",
       "  5840,\n",
       "  7377,\n",
       "  4,\n",
       "  82,\n",
       "  79,\n",
       "  10,\n",
       "  11,\n",
       "  2,\n",
       "  361,\n",
       "  107,\n",
       "  10487,\n",
       "  4,\n",
       "  48,\n",
       "  9,\n",
       "  100,\n",
       "  546,\n",
       "  141,\n",
       "  16,\n",
       "  10043,\n",
       "  5,\n",
       "  582,\n",
       "  7,\n",
       "  16912,\n",
       "  8,\n",
       "  2,\n",
       "  3998,\n",
       "  0,\n",
       "  15,\n",
       "  12243,\n",
       "  15,\n",
       "  4,\n",
       "  2,\n",
       "  210,\n",
       "  14,\n",
       "  2,\n",
       "  17041,\n",
       "  45,\n",
       "  10679,\n",
       "  29,\n",
       "  3522,\n",
       "  5891,\n",
       "  30,\n",
       "  55,\n",
       "  15,\n",
       "  93,\n",
       "  2821,\n",
       "  15,\n",
       "  9,\n",
       "  1021,\n",
       "  8,\n",
       "  286,\n",
       "  104,\n",
       "  27,\n",
       "  2,\n",
       "  184,\n",
       "  699,\n",
       "  21,\n",
       "  2,\n",
       "  0,\n",
       "  44,\n",
       "  100,\n",
       "  35,\n",
       "  93,\n",
       "  2821,\n",
       "  4,\n",
       "  1227,\n",
       "  3,\n",
       "  2,\n",
       "  1688,\n",
       "  7,\n",
       "  131,\n",
       "  5,\n",
       "  347,\n",
       "  32719,\n",
       "  15,\n",
       "  8887,\n",
       "  141,\n",
       "  16,\n",
       "  15,\n",
       "  0,\n",
       "  53,\n",
       "  15,\n",
       "  2,\n",
       "  1167,\n",
       "  15,\n",
       "  4,\n",
       "  13,\n",
       "  14398,\n",
       "  1248,\n",
       "  688,\n",
       "  113,\n",
       "  535,\n",
       "  22,\n",
       "  2,\n",
       "  529,\n",
       "  3,\n",
       "  24,\n",
       "  3462,\n",
       "  31,\n",
       "  535,\n",
       "  213,\n",
       "  22,\n",
       "  2,\n",
       "  119,\n",
       "  4],\n",
       " 'TEXT_LENGTH': 182}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'a',\n",
       " '<unk>',\n",
       " 'club',\n",
       " 'and',\n",
       " 'i',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'that',\n",
       " 'deliverance',\n",
       " 'is',\n",
       " '<unk>',\n",
       " 'with',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'and',\n",
       " 'tranquility',\n",
       " 'of',\n",
       " 'the',\n",
       " 'experience',\n",
       " '.',\n",
       " 'as',\n",
       " 'we',\n",
       " 'put',\n",
       " 'our',\n",
       " 'boats',\n",
       " 'into',\n",
       " 'the',\n",
       " 'water',\n",
       " ',',\n",
       " '<unk>',\n",
       " 'echo',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'the',\n",
       " 'conscious',\n",
       " 'mind',\n",
       " '.',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'timeless',\n",
       " 'because',\n",
       " 'it',\n",
       " 'waxes',\n",
       " 'philosophical',\n",
       " 'of',\n",
       " 'human',\n",
       " \"'s\",\n",
       " 'place',\n",
       " 'in',\n",
       " 'nature',\n",
       " 'and',\n",
       " 'technology',\n",
       " \"'s\",\n",
       " 'effect',\n",
       " 'upon',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'relationship',\n",
       " 'with',\n",
       " 'nature',\n",
       " '.',\n",
       " 'we',\n",
       " 'see',\n",
       " 'it',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bow',\n",
       " 'fishing',\n",
       " '.',\n",
       " 'we',\n",
       " 'see',\n",
       " 'it',\n",
       " 'in',\n",
       " 'the',\n",
       " 'home',\n",
       " 'made',\n",
       " 'tent',\n",
       " '.',\n",
       " 'there',\n",
       " 'is',\n",
       " 'also',\n",
       " 'city',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'disdain',\n",
       " 'and',\n",
       " 'feeling',\n",
       " 'of',\n",
       " 'superiority',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rural',\n",
       " '<unk>',\n",
       " '\"',\n",
       " 'cracker',\n",
       " '\"',\n",
       " '.',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'the',\n",
       " 'banker',\n",
       " 'from',\n",
       " 'atlanta',\n",
       " '(',\n",
       " 'ned',\n",
       " 'beatty',\n",
       " ')',\n",
       " 'has',\n",
       " '\"',\n",
       " 'bad',\n",
       " 'teeth',\n",
       " '\"',\n",
       " 'is',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'put',\n",
       " 'him',\n",
       " 'on',\n",
       " 'the',\n",
       " 'same',\n",
       " 'level',\n",
       " 'with',\n",
       " 'the',\n",
       " '<unk>',\n",
       " 'who',\n",
       " 'also',\n",
       " 'have',\n",
       " 'bad',\n",
       " 'teeth',\n",
       " '.',\n",
       " 'ultimately',\n",
       " ',',\n",
       " 'the',\n",
       " 'struggle',\n",
       " 'of',\n",
       " 'life',\n",
       " 'and',\n",
       " 'death',\n",
       " 'supersedes',\n",
       " '\"',\n",
       " 'civilized',\n",
       " 'man',\n",
       " \"'s\",\n",
       " '\"',\n",
       " '<unk>',\n",
       " 'about',\n",
       " '\"',\n",
       " 'the',\n",
       " 'law',\n",
       " '\"',\n",
       " '.',\n",
       " 'this',\n",
       " 'canoe',\n",
       " 'trip',\n",
       " 'ends',\n",
       " 'too',\n",
       " 'soon',\n",
       " 'for',\n",
       " 'the',\n",
       " 'viewer',\n",
       " ',',\n",
       " 'but',\n",
       " 'alas',\n",
       " 'not',\n",
       " 'soon',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'the',\n",
       " 'characters',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.lookup_tokens(train_data[0][\"token_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'token_ids': 'TEXT_COLUMN_NAME',\n",
    "    'sentiment': 'LABEL_COLUMN_NAME'\n",
    "}\n",
    "\n",
    "train_data = train_data.rename_columns(column_mapping)\n",
    "valid_data = valid_data.rename_columns(column_mapping)\n",
    "test_data = test_data.rename_columns(column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'I am a member of a canoeing club and I can tell you the truth that Deliverance is synonomous with the peacefulness and tranquility of the experience. As we put our boats into the water, banjoes echo in the back of the conscious mind. This movie is timeless because it waxes philosophical of human\\'s place in nature and technology\\'s effect upon man\\'s relationship with nature. We see it in the bow fishing. We see it in the home made tent. There is also city man\\'s disdain and feeling of superiority to the rural woodsman \"cracker\". The fact that the Banker from Atlanta (Ned Beatty) has \"bad teeth\" is meant to put him on the same level with the woodsmen who also have bad teeth. Ultimately, the struggle of life and death supersedes \"civilized man\\'s\" suppositives about \"The Law\". This canoe trip ends too soon for the viewer, but alas Not Soon Enough for the characters.',\n",
       " 'LABEL_COLUMN_NAME': 1,\n",
       " 'tokens': ['i',\n",
       "  'am',\n",
       "  'a',\n",
       "  'member',\n",
       "  'of',\n",
       "  'a',\n",
       "  'canoeing',\n",
       "  'club',\n",
       "  'and',\n",
       "  'i',\n",
       "  'can',\n",
       "  'tell',\n",
       "  'you',\n",
       "  'the',\n",
       "  'truth',\n",
       "  'that',\n",
       "  'deliverance',\n",
       "  'is',\n",
       "  'synonomous',\n",
       "  'with',\n",
       "  'the',\n",
       "  'peacefulness',\n",
       "  'and',\n",
       "  'tranquility',\n",
       "  'of',\n",
       "  'the',\n",
       "  'experience',\n",
       "  '.',\n",
       "  'as',\n",
       "  'we',\n",
       "  'put',\n",
       "  'our',\n",
       "  'boats',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  ',',\n",
       "  'banjoes',\n",
       "  'echo',\n",
       "  'in',\n",
       "  'the',\n",
       "  'back',\n",
       "  'of',\n",
       "  'the',\n",
       "  'conscious',\n",
       "  'mind',\n",
       "  '.',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'timeless',\n",
       "  'because',\n",
       "  'it',\n",
       "  'waxes',\n",
       "  'philosophical',\n",
       "  'of',\n",
       "  'human',\n",
       "  \"'s\",\n",
       "  'place',\n",
       "  'in',\n",
       "  'nature',\n",
       "  'and',\n",
       "  'technology',\n",
       "  \"'s\",\n",
       "  'effect',\n",
       "  'upon',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'relationship',\n",
       "  'with',\n",
       "  'nature',\n",
       "  '.',\n",
       "  'we',\n",
       "  'see',\n",
       "  'it',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bow',\n",
       "  'fishing',\n",
       "  '.',\n",
       "  'we',\n",
       "  'see',\n",
       "  'it',\n",
       "  'in',\n",
       "  'the',\n",
       "  'home',\n",
       "  'made',\n",
       "  'tent',\n",
       "  '.',\n",
       "  'there',\n",
       "  'is',\n",
       "  'also',\n",
       "  'city',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  'disdain',\n",
       "  'and',\n",
       "  'feeling',\n",
       "  'of',\n",
       "  'superiority',\n",
       "  'to',\n",
       "  'the',\n",
       "  'rural',\n",
       "  'woodsman',\n",
       "  '\"',\n",
       "  'cracker',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'the',\n",
       "  'banker',\n",
       "  'from',\n",
       "  'atlanta',\n",
       "  '(',\n",
       "  'ned',\n",
       "  'beatty',\n",
       "  ')',\n",
       "  'has',\n",
       "  '\"',\n",
       "  'bad',\n",
       "  'teeth',\n",
       "  '\"',\n",
       "  'is',\n",
       "  'meant',\n",
       "  'to',\n",
       "  'put',\n",
       "  'him',\n",
       "  'on',\n",
       "  'the',\n",
       "  'same',\n",
       "  'level',\n",
       "  'with',\n",
       "  'the',\n",
       "  'woodsmen',\n",
       "  'who',\n",
       "  'also',\n",
       "  'have',\n",
       "  'bad',\n",
       "  'teeth',\n",
       "  '.',\n",
       "  'ultimately',\n",
       "  ',',\n",
       "  'the',\n",
       "  'struggle',\n",
       "  'of',\n",
       "  'life',\n",
       "  'and',\n",
       "  'death',\n",
       "  'supersedes',\n",
       "  '\"',\n",
       "  'civilized',\n",
       "  'man',\n",
       "  \"'s\",\n",
       "  '\"',\n",
       "  'suppositives',\n",
       "  'about',\n",
       "  '\"',\n",
       "  'the',\n",
       "  'law',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'this',\n",
       "  'canoe',\n",
       "  'trip',\n",
       "  'ends',\n",
       "  'too',\n",
       "  'soon',\n",
       "  'for',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  ',',\n",
       "  'but',\n",
       "  'alas',\n",
       "  'not',\n",
       "  'soon',\n",
       "  'enough',\n",
       "  'for',\n",
       "  'the',\n",
       "  'characters',\n",
       "  '.'],\n",
       " 'TEXT_COLUMN_NAME': [12,\n",
       "  244,\n",
       "  6,\n",
       "  1727,\n",
       "  7,\n",
       "  6,\n",
       "  0,\n",
       "  1384,\n",
       "  5,\n",
       "  12,\n",
       "  72,\n",
       "  393,\n",
       "  26,\n",
       "  2,\n",
       "  884,\n",
       "  14,\n",
       "  7581,\n",
       "  9,\n",
       "  0,\n",
       "  21,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  26161,\n",
       "  7,\n",
       "  2,\n",
       "  599,\n",
       "  4,\n",
       "  20,\n",
       "  82,\n",
       "  286,\n",
       "  273,\n",
       "  9402,\n",
       "  97,\n",
       "  2,\n",
       "  1003,\n",
       "  3,\n",
       "  0,\n",
       "  9157,\n",
       "  11,\n",
       "  2,\n",
       "  160,\n",
       "  7,\n",
       "  2,\n",
       "  5528,\n",
       "  351,\n",
       "  4,\n",
       "  13,\n",
       "  23,\n",
       "  9,\n",
       "  3914,\n",
       "  98,\n",
       "  10,\n",
       "  28001,\n",
       "  4486,\n",
       "  7,\n",
       "  405,\n",
       "  16,\n",
       "  294,\n",
       "  11,\n",
       "  919,\n",
       "  5,\n",
       "  2192,\n",
       "  16,\n",
       "  942,\n",
       "  695,\n",
       "  141,\n",
       "  16,\n",
       "  659,\n",
       "  21,\n",
       "  919,\n",
       "  4,\n",
       "  82,\n",
       "  79,\n",
       "  10,\n",
       "  11,\n",
       "  2,\n",
       "  5840,\n",
       "  7377,\n",
       "  4,\n",
       "  82,\n",
       "  79,\n",
       "  10,\n",
       "  11,\n",
       "  2,\n",
       "  361,\n",
       "  107,\n",
       "  10487,\n",
       "  4,\n",
       "  48,\n",
       "  9,\n",
       "  100,\n",
       "  546,\n",
       "  141,\n",
       "  16,\n",
       "  10043,\n",
       "  5,\n",
       "  582,\n",
       "  7,\n",
       "  16912,\n",
       "  8,\n",
       "  2,\n",
       "  3998,\n",
       "  0,\n",
       "  15,\n",
       "  12243,\n",
       "  15,\n",
       "  4,\n",
       "  2,\n",
       "  210,\n",
       "  14,\n",
       "  2,\n",
       "  17041,\n",
       "  45,\n",
       "  10679,\n",
       "  29,\n",
       "  3522,\n",
       "  5891,\n",
       "  30,\n",
       "  55,\n",
       "  15,\n",
       "  93,\n",
       "  2821,\n",
       "  15,\n",
       "  9,\n",
       "  1021,\n",
       "  8,\n",
       "  286,\n",
       "  104,\n",
       "  27,\n",
       "  2,\n",
       "  184,\n",
       "  699,\n",
       "  21,\n",
       "  2,\n",
       "  0,\n",
       "  44,\n",
       "  100,\n",
       "  35,\n",
       "  93,\n",
       "  2821,\n",
       "  4,\n",
       "  1227,\n",
       "  3,\n",
       "  2,\n",
       "  1688,\n",
       "  7,\n",
       "  131,\n",
       "  5,\n",
       "  347,\n",
       "  32719,\n",
       "  15,\n",
       "  8887,\n",
       "  141,\n",
       "  16,\n",
       "  15,\n",
       "  0,\n",
       "  53,\n",
       "  15,\n",
       "  2,\n",
       "  1167,\n",
       "  15,\n",
       "  4,\n",
       "  13,\n",
       "  14398,\n",
       "  1248,\n",
       "  688,\n",
       "  113,\n",
       "  535,\n",
       "  22,\n",
       "  2,\n",
       "  529,\n",
       "  3,\n",
       "  24,\n",
       "  3462,\n",
       "  31,\n",
       "  535,\n",
       "  213,\n",
       "  22,\n",
       "  2,\n",
       "  119,\n",
       "  4],\n",
       " 'TEXT_LENGTH': 182}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing that the `datasets` library handles for us with the `Dataset` class is converting features to the correct type. Our indices in each example are currently basic Python integers. However, they need to be converted to PyTorch tensors in order to use them with PyTorch. We could convert them just before we pass them into the model, however it is more convenient to do it now.\n",
    "\n",
    "The `with_format` method converts features indicated by the columns argument to a given type. Here, we specify the type as \"torch\" (for PyTorch) and the columns to be \"TEXT_COLUMN_NAME\" and \"LABEL_COLUMN_NAME\" (the features which we want to convert to PyTorch tensors). By default, `with_format` will remove any features not in the list of features passed to columns. If we want to keep those features, we can do with `output_all_columns=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"TEXT_COLUMN_NAME\", \"LABEL_COLUMN_NAME\", \"TEXT_LENGTH\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    # output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    #output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    #output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LABEL_COLUMN_NAME': tensor(1),\n",
       " 'TEXT_LENGTH': tensor(182),\n",
       " 'TEXT_COLUMN_NAME': tensor([   12,   244,     6,  1727,     7,     6,     0,  1384,     5,    12,\n",
       "            72,   393,    26,     2,   884,    14,  7581,     9,     0,    21,\n",
       "             2,     0,     5, 26161,     7,     2,   599,     4,    20,    82,\n",
       "           286,   273,  9402,    97,     2,  1003,     3,     0,  9157,    11,\n",
       "             2,   160,     7,     2,  5528,   351,     4,    13,    23,     9,\n",
       "          3914,    98,    10, 28001,  4486,     7,   405,    16,   294,    11,\n",
       "           919,     5,  2192,    16,   942,   695,   141,    16,   659,    21,\n",
       "           919,     4,    82,    79,    10,    11,     2,  5840,  7377,     4,\n",
       "            82,    79,    10,    11,     2,   361,   107, 10487,     4,    48,\n",
       "             9,   100,   546,   141,    16, 10043,     5,   582,     7, 16912,\n",
       "             8,     2,  3998,     0,    15, 12243,    15,     4,     2,   210,\n",
       "            14,     2, 17041,    45, 10679,    29,  3522,  5891,    30,    55,\n",
       "            15,    93,  2821,    15,     9,  1021,     8,   286,   104,    27,\n",
       "             2,   184,   699,    21,     2,     0,    44,   100,    35,    93,\n",
       "          2821,     4,  1227,     3,     2,  1688,     7,   131,     5,   347,\n",
       "         32719,    15,  8887,   141,    16,    15,     0,    53,    15,     2,\n",
       "          1167,    15,     4,    13, 14398,  1248,   688,   113,   535,    22,\n",
       "             2,   529,     3,    24,  3462,    31,   535,   213,    22,     2,\n",
       "           119,     4])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sort(column_names=['TEXT_LENGTH'], reverse=True)\n",
    "valid_data = valid_data.sort(column_names=['TEXT_LENGTH'], reverse=True)\n",
    "test_data = test_data.sort(column_names=['TEXT_LENGTH'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LABEL_COLUMN_NAME': tensor(1),\n",
       " 'TEXT_LENGTH': tensor(2789),\n",
       " 'TEXT_COLUMN_NAME': tensor([1091,  521,   98,  ...,   18,  161,  533])}\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIQ_zfKLwjKm"
   },
   "source": [
    "## Define Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7JiHR1stHNF"
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_text_ids = [data[\"TEXT_COLUMN_NAME\"] for data in batch]\n",
    "        batch_text_ids = torch.nn.utils.rnn.pad_sequence(batch_text_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"TEXT_COLUMN_NAME\": batch_text_ids,\n",
    "            \"LABEL_COLUMN_NAME\": torch.tensor([data[\"LABEL_COLUMN_NAME\"] for data in batch]),\n",
    "            \"TEXT_LENGTH\": torch.tensor([data[\"TEXT_LENGTH\"] for data in batch])\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "train_loader = get_data_loader(train_data, BATCH_SIZE, pad_index) # NEW\n",
    "valid_loader = get_data_loader(valid_data, BATCH_SIZE, pad_index)\n",
    "test_loader = get_data_loader(test_data, BATCH_SIZE, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0pT_dMRvicQ"
   },
   "source": [
    "Testing the iterators (note that the number of rows depends on the longest document in the respective batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "y8SP_FccutT0",
    "outputId": "fe33763a-4560-4dee-adee-31cc6c48b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([1136, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "Text Length vector size: torch.Size([128])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([55, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "Text Length vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([52, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "Text Length vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch[\"TEXT_COLUMN_NAME\"].size()}')\n",
    "    print(f'Target vector size: {batch[\"LABEL_COLUMN_NAME\"].size()}')\n",
    "    print(f'Text Length vector size: {batch[\"TEXT_LENGTH\"].size()}')\n",
    "    break\n",
    "\n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch[\"TEXT_COLUMN_NAME\"].size()}')\n",
    "    print(f'Target vector size: {batch[\"LABEL_COLUMN_NAME\"].size()}')\n",
    "    print(f'Text Length vector size: {batch[\"TEXT_LENGTH\"].size()}')\n",
    "    break\n",
    "\n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch[\"TEXT_COLUMN_NAME\"].size()}')\n",
    "    print(f'Target vector size: {batch[\"LABEL_COLUMN_NAME\"].size()}')\n",
    "    print(f'Text Length vector size: {batch[\"TEXT_LENGTH\"].size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_grdW3pxCzz"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQIUm5EjxFNa"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        #self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "        # text dim: [sentence length, batch size]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "\n",
    "        ## NEW\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'))\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(packed)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik3NF3faxFmZ"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = RNN(input_dim=len(en_vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lv9Ny9di6VcI"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5t1Afn4xO11"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for batch_data in data_loader:\n",
    "            features = batch_data[\"TEXT_COLUMN_NAME\"]\n",
    "            text_length = batch_data[\"TEXT_LENGTH\"] # NEW\n",
    "            targets = batch_data[\"LABEL_COLUMN_NAME\"]\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features, text_length)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1836
    },
    "colab_type": "code",
    "id": "EABZM8Vo0ilB",
    "outputId": "5d45e293-9909-4588-e793-8dfaf72e5c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/266 | Loss: 0.7058\n",
      "Epoch: 001/015 | Batch 050/266 | Loss: 0.6915\n",
      "Epoch: 001/015 | Batch 100/266 | Loss: 0.6958\n",
      "Epoch: 001/015 | Batch 150/266 | Loss: 0.6933\n",
      "Epoch: 001/015 | Batch 200/266 | Loss: 0.6998\n",
      "Epoch: 001/015 | Batch 250/266 | Loss: 0.6887\n",
      "training accuracy: 50.08%\n",
      "valid accuracy: 49.23%\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 002/015 | Batch 000/266 | Loss: 0.6926\n",
      "Epoch: 002/015 | Batch 050/266 | Loss: 0.6938\n",
      "Epoch: 002/015 | Batch 100/266 | Loss: 0.6937\n",
      "Epoch: 002/015 | Batch 150/266 | Loss: 0.6915\n",
      "Epoch: 002/015 | Batch 200/266 | Loss: 0.6900\n",
      "Epoch: 002/015 | Batch 250/266 | Loss: 0.6904\n",
      "training accuracy: 50.16%\n",
      "valid accuracy: 51.15%\n",
      "Time elapsed: 1.60 min\n",
      "Epoch: 003/015 | Batch 000/266 | Loss: 0.6902\n",
      "Epoch: 003/015 | Batch 050/266 | Loss: 0.6932\n",
      "Epoch: 003/015 | Batch 100/266 | Loss: 0.7039\n",
      "Epoch: 003/015 | Batch 150/266 | Loss: 0.6932\n",
      "Epoch: 003/015 | Batch 200/266 | Loss: 0.6928\n",
      "Epoch: 003/015 | Batch 250/266 | Loss: 0.6915\n",
      "training accuracy: 50.20%\n",
      "valid accuracy: 51.05%\n",
      "Time elapsed: 2.42 min\n",
      "Epoch: 004/015 | Batch 000/266 | Loss: 0.6875\n",
      "Epoch: 004/015 | Batch 050/266 | Loss: 0.6945\n",
      "Epoch: 004/015 | Batch 100/266 | Loss: 0.6915\n",
      "Epoch: 004/015 | Batch 150/266 | Loss: 0.6898\n",
      "Epoch: 004/015 | Batch 200/266 | Loss: 0.6947\n",
      "Epoch: 004/015 | Batch 250/266 | Loss: 0.6876\n",
      "training accuracy: 50.28%\n",
      "valid accuracy: 49.78%\n",
      "Time elapsed: 3.24 min\n",
      "Epoch: 005/015 | Batch 000/266 | Loss: 0.6892\n",
      "Epoch: 005/015 | Batch 050/266 | Loss: 0.6878\n",
      "Epoch: 005/015 | Batch 100/266 | Loss: 0.6899\n",
      "Epoch: 005/015 | Batch 150/266 | Loss: 0.6912\n",
      "Epoch: 005/015 | Batch 200/266 | Loss: 0.6886\n",
      "Epoch: 005/015 | Batch 250/266 | Loss: 0.6906\n",
      "training accuracy: 50.27%\n",
      "valid accuracy: 49.83%\n",
      "Time elapsed: 4.05 min\n",
      "Epoch: 006/015 | Batch 000/266 | Loss: 0.6918\n",
      "Epoch: 006/015 | Batch 050/266 | Loss: 0.6880\n",
      "Epoch: 006/015 | Batch 100/266 | Loss: 0.6883\n",
      "Epoch: 006/015 | Batch 150/266 | Loss: 0.6925\n",
      "Epoch: 006/015 | Batch 200/266 | Loss: 0.6903\n",
      "Epoch: 006/015 | Batch 250/266 | Loss: 0.7060\n",
      "training accuracy: 50.26%\n",
      "valid accuracy: 51.52%\n",
      "Time elapsed: 4.86 min\n",
      "Epoch: 007/015 | Batch 000/266 | Loss: 0.6895\n",
      "Epoch: 007/015 | Batch 050/266 | Loss: 0.6912\n",
      "Epoch: 007/015 | Batch 100/266 | Loss: 0.7000\n",
      "Epoch: 007/015 | Batch 150/266 | Loss: 0.6886\n",
      "Epoch: 007/015 | Batch 200/266 | Loss: 0.6867\n",
      "Epoch: 007/015 | Batch 250/266 | Loss: 0.6889\n",
      "training accuracy: 50.28%\n",
      "valid accuracy: 52.47%\n",
      "Time elapsed: 5.69 min\n",
      "Epoch: 008/015 | Batch 000/266 | Loss: 0.6882\n",
      "Epoch: 008/015 | Batch 050/266 | Loss: 0.6880\n",
      "Epoch: 008/015 | Batch 100/266 | Loss: 0.6879\n",
      "Epoch: 008/015 | Batch 150/266 | Loss: 0.6889\n",
      "Epoch: 008/015 | Batch 200/266 | Loss: 0.6466\n",
      "Epoch: 008/015 | Batch 250/266 | Loss: 0.6653\n",
      "training accuracy: 67.55%\n",
      "valid accuracy: 64.38%\n",
      "Time elapsed: 6.51 min\n",
      "Epoch: 009/015 | Batch 000/266 | Loss: 0.6419\n",
      "Epoch: 009/015 | Batch 050/266 | Loss: 0.6600\n",
      "Epoch: 009/015 | Batch 100/266 | Loss: 0.5552\n",
      "Epoch: 009/015 | Batch 150/266 | Loss: 0.5692\n",
      "Epoch: 009/015 | Batch 200/266 | Loss: 0.4600\n",
      "Epoch: 009/015 | Batch 250/266 | Loss: 0.4351\n",
      "training accuracy: 81.11%\n",
      "valid accuracy: 78.88%\n",
      "Time elapsed: 7.32 min\n",
      "Epoch: 010/015 | Batch 000/266 | Loss: 0.4393\n",
      "Epoch: 010/015 | Batch 050/266 | Loss: 0.4834\n",
      "Epoch: 010/015 | Batch 100/266 | Loss: 0.4660\n",
      "Epoch: 010/015 | Batch 150/266 | Loss: 0.4124\n",
      "Epoch: 010/015 | Batch 200/266 | Loss: 0.4119\n",
      "Epoch: 010/015 | Batch 250/266 | Loss: 0.4837\n",
      "training accuracy: 86.55%\n",
      "valid accuracy: 82.45%\n",
      "Time elapsed: 8.13 min\n",
      "Epoch: 011/015 | Batch 000/266 | Loss: 0.2913\n",
      "Epoch: 011/015 | Batch 050/266 | Loss: 0.2869\n",
      "Epoch: 011/015 | Batch 100/266 | Loss: 0.2748\n",
      "Epoch: 011/015 | Batch 150/266 | Loss: 0.3801\n",
      "Epoch: 011/015 | Batch 200/266 | Loss: 0.3037\n",
      "Epoch: 011/015 | Batch 250/266 | Loss: 0.3423\n",
      "training accuracy: 86.65%\n",
      "valid accuracy: 82.47%\n",
      "Time elapsed: 8.96 min\n",
      "Epoch: 012/015 | Batch 000/266 | Loss: 0.3412\n",
      "Epoch: 012/015 | Batch 050/266 | Loss: 0.3743\n",
      "Epoch: 012/015 | Batch 100/266 | Loss: 0.3860\n",
      "Epoch: 012/015 | Batch 150/266 | Loss: 0.2885\n",
      "Epoch: 012/015 | Batch 200/266 | Loss: 0.2734\n",
      "Epoch: 012/015 | Batch 250/266 | Loss: 0.2832\n",
      "training accuracy: 88.88%\n",
      "valid accuracy: 83.88%\n",
      "Time elapsed: 9.78 min\n",
      "Epoch: 013/015 | Batch 000/266 | Loss: 0.3068\n",
      "Epoch: 013/015 | Batch 050/266 | Loss: 0.3469\n",
      "Epoch: 013/015 | Batch 100/266 | Loss: 0.2812\n",
      "Epoch: 013/015 | Batch 150/266 | Loss: 0.2410\n",
      "Epoch: 013/015 | Batch 200/266 | Loss: 0.1853\n",
      "Epoch: 013/015 | Batch 250/266 | Loss: 0.1869\n",
      "training accuracy: 91.21%\n",
      "valid accuracy: 84.05%\n",
      "Time elapsed: 10.60 min\n",
      "Epoch: 014/015 | Batch 000/266 | Loss: 0.2667\n",
      "Epoch: 014/015 | Batch 050/266 | Loss: 0.3763\n",
      "Epoch: 014/015 | Batch 100/266 | Loss: 0.1946\n",
      "Epoch: 014/015 | Batch 150/266 | Loss: 0.2481\n",
      "Epoch: 014/015 | Batch 200/266 | Loss: 0.2215\n",
      "Epoch: 014/015 | Batch 250/266 | Loss: 0.3347\n",
      "training accuracy: 92.18%\n",
      "valid accuracy: 85.48%\n",
      "Time elapsed: 11.43 min\n",
      "Epoch: 015/015 | Batch 000/266 | Loss: 0.2571\n",
      "Epoch: 015/015 | Batch 050/266 | Loss: 0.2076\n",
      "Epoch: 015/015 | Batch 100/266 | Loss: 0.2635\n",
      "Epoch: 015/015 | Batch 150/266 | Loss: 0.2632\n",
      "Epoch: 015/015 | Batch 200/266 | Loss: 0.2289\n",
      "Epoch: 015/015 | Batch 250/266 | Loss: 0.2511\n",
      "training accuracy: 92.54%\n",
      "valid accuracy: 85.63%\n",
      "Time elapsed: 12.24 min\n",
      "Total Training Time: 12.24 min\n",
      "Test accuracy: 84.20%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "\n",
    "        text = batch_data[\"TEXT_COLUMN_NAME\"].to(DEVICE)\n",
    "        text_length = batch_data[\"TEXT_LENGTH\"] # NEW\n",
    "        labels = batch_data[\"LABEL_COLUMN_NAME\"].to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(text, text_length)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "\n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jt55pscgFdKZ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def predict(model, sentence):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "        indexed = [en_vocab[t.lower()] for t in tokenized]\n",
    "        length = [len(indexed)]\n",
    "        tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        length_tensor = torch.LongTensor(length)\n",
    "        predict_probas = torch.nn.functional.softmax(model(tensor, length_tensor), dim=1)\n",
    "        predicted_label_index = torch.argmax(predict_probas)\n",
    "        predicted_label_proba = torch.max(predict_probas)\n",
    "        # classes: {0: Negative, 1: Positive}\n",
    "        return predicted_label_index.item(), predicted_label_proba.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability positive:\n",
      "Predicted label index: 0 | Predicted label: 1 | Probability: 0.9999784231185913\n"
     ]
    }
   ],
   "source": [
    "print('Probability positive:')\n",
    "\n",
    "predicted_label, predicted_label_proba = \\\n",
    "    predict(model, \"This is such an awesome movie, I really love it!\")\n",
    "\n",
    "print(f'Predicted label: {predicted_label}'\n",
    "      f' | Probability: {predicted_label_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability negative:\n",
      "Predicted label index: 1 | Predicted label: 0 | Probability: 0.9999545812606812\n"
     ]
    }
   ],
   "source": [
    "print('Probability negative:')\n",
    "\n",
    "predicted_label, predicted_label_proba = \\\n",
    "    predict(model, \"I really hate this movie. It is really bad and sucks!\")\n",
    "\n",
    "print(f'Predicted label index: {predicted_label}'\n",
    "      f' | Probability: {predicted_label_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lRusB3dF80X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas   : 2.2.2\n",
      "spacy    : 3.7.3\n",
      "torch    : 2.3.0\n",
      "torchtext: 0.18.0\n",
      "datasets : 3.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_lstm_packed_imdb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
